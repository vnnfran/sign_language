Classes: 
* I love you, yes, no, hello, sorry

Used adapted ASL signs from paper:
https://www.researchgate.net/figure/ASL-static-word-signs_fig3_337285019

Main libraries used:
* OpenCV: Get real-time video input and manage accordingly.
* CVZONE: Use the Hand Tracking module; Mediapipe wrapper to simplify use.
* Keras / TF: Train an ANN to recognize some sign language gestures.

Project phases:
1. Set up virtual environment. - Done
2. Install required libraries. - Done
3. Test camera connection. - Done
4. Configure mediapipe. - Done
5. Test hand msarker recognition. - Done
6. Code script for picture collection. - Done
7. Take pictures. - Done
8. Code script for ANN training. - Done
9. Train ANN. - In progress
10. Test ANN. - In progress
11. Set up real-time implementation.
12. Tests.
13. Presentation.
Extra: Repeat for a CNN model and compare.

Activating the venv:
.venv\Scripts\activate

Run files directly in the Python terminal using:
python {filename}.py